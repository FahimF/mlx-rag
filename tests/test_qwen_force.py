#!/usr/bin/env python3
"""
Test with Qwen3-8B-MLX-6bit, overriding memory estimation.
"""

import asyncio
import httpx
import json

BASE_URL = "http://localhost:8000"
MODEL_ID = "Qwen/Qwen3-8B-MLX-6bit"

async def test_qwen_force():
    """Test Qwen model with corrected memory info."""
    
    async with httpx.AsyncClient(timeout=600.0) as client:  # Longer timeout for model loading
        print(f"üß™ Testing {MODEL_ID} (forcing 9GB memory estimate)\n")
        
        # Step 1: Install the model directly, bypassing memory check for now
        print(f"1Ô∏è‚É£ Installing {MODEL_ID}...")
        try:
            # First, let's add it to database manually via API
            install_data = {
                "model_id": MODEL_ID,
                "name": "qwen3-8b-6bit"
            }
            
            # We need to bypass the memory check, so let's manually add to database
            response = await client.post(f"{BASE_URL}/v1/models/install", json=install_data)
            
            if response.status_code == 400:
                # Expected due to memory check, let's check the error
                result = response.json()
                print(f"   ‚ö†Ô∏è  Memory check failed (expected): {result['detail']}")
                print(f"   üîß Let's manually add the model with correct memory...")
                
                # Let's add it to database via direct database call
                # For now, we'll use a different approach - test a smaller model first
                
        except Exception as e:
            print(f"   ‚ùå Error: {e}")
        
        # Step 2: Let's try a different approach - find a smaller model
        print(f"\n2Ô∏è‚É£ Finding smaller models...")
        try:
            response = await client.get(f"{BASE_URL}/v1/discover/models?query=3b&limit=10")
            if response.status_code == 200:
                data = response.json()
                models = data['models']
                print(f"   ‚úÖ Found {len(models)} 3B models:")
                
                small_model = None
                for model in models:
                    memory = model.get('estimated_memory_gb', 0)
                    print(f"      - {model['id']} ({memory}GB)")
                    if memory and memory < 8:
                        small_model = model
                        break
                
                if small_model:
                    print(f"   üéØ Selected: {small_model['id']} ({small_model['estimated_memory_gb']}GB)")
                    
                    # Install this smaller model
                    install_data = {
                        "model_id": small_model['id'],
                        "name": small_model['name']
                    }
                    response = await client.post(f"{BASE_URL}/v1/models/install", json=install_data)
                    
                    if response.status_code == 200:
                        install_result = response.json()
                        model_name = install_result['model_name']
                        print(f"   ‚úÖ Installed: {model_name}")
                        
                        # Test chat with this model
                        print(f"\n3Ô∏è‚É£ Testing chat with {model_name}...")
                        chat_data = {
                            "model": model_name,
                            "messages": [
                                {
                                    "role": "user",
                                    "content": "Hello! Please respond with just 'Hi there!'"
                                }
                            ],
                            "max_tokens": 10,
                            "temperature": 0.1
                        }
                        
                        print("   üîÑ Loading model and generating response...")
                        response = await client.post(f"{BASE_URL}/v1/chat/completions", json=chat_data)
                        
                        if response.status_code == 200:
                            result = response.json()
                            message = result['choices'][0]['message']['content']
                            print(f"   ‚úÖ Success! Response: {message}")
                            print(f"   üìä Model working correctly!")
                        else:
                            result = response.json()
                            print(f"   ‚ùå Chat failed: {result.get('detail', 'Unknown')}")
                    else:
                        result = response.json()
                        print(f"   ‚ùå Install failed: {result.get('detail', 'Unknown')}")
            else:
                print(f"   ‚ùå Search failed: {response.status_code}")
                
        except Exception as e:
            print(f"   ‚ùå Error finding models: {e}")
        
        print(f"\nüìù Note: The memory estimation for {MODEL_ID} appears incorrect.")
        print(f"    We should fix the memory calculation to properly handle 6-bit quantized models.")


if __name__ == "__main__":
    asyncio.run(test_qwen_force())