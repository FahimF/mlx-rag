# Chat Integration

New Tool Call Detection and Parsing Utilities

1. _parse_tool_calls_from_text(text: str) Function
This function detects and parses tool calls from LLM responses in multiple formats:

‚Ä¢  JSON-style function calls: {"function": "tool_name", "arguments": {...}}
‚Ä¢  XML-style tool calls: <tool_call function="tool_name" args='{"key": "value"}'/>  
‚Ä¢  Function call patterns: tool_name({"key": "value"})

Key features:
‚Ä¢  Uses regex patterns to detect different tool call formats
‚Ä¢  Validates JSON arguments before creating tool calls
‚Ä¢  Generates unique IDs for each tool call
‚Ä¢  Removes duplicate tool calls
‚Ä¢  Only processes known tool names to avoid false positives
‚Ä¢  Comprehensive logging for debugging

2. _create_system_prompt_with_tools(tools: List[Dict[str, Any]]) Function
Creates system prompts that instruct LLMs on how to use available tools:

‚Ä¢  Generates detailed tool descriptions with parameters
‚Ä¢  Includes parameter types, descriptions, and requirement status
‚Ä¢  Provides clear JSON format instructions for tool usage
‚Ä¢  Returns a helpful default prompt when no tools are provided

Usage in Chat Completions Endpoint

These utilities integrate seamlessly with your existing /v1/chat/completions endpoint. You can now:

1. Detect tool calls in responses: The _parse_tool_calls_from_text() function can be called on LLM responses to extract structured tool calls
2. Add tool support: Use the _create_system_prompt_with_tools() function to modify system prompts when tools are provided in requests
3. Handle tool choice: The existing Pydantic models already support tools and tool_choice parameters

Example Integration

Here's how you could use these utilities in your chat completions endpoint:

```python
# If tools are provided in the request
if request.tools:
    # Create enhanced system prompt
    system_prompt = _create_system_prompt_with_tools(request.tools)
    # Add or modify system message in messages
    
# After getting LLM response
response_text = "LLM generated response..."
detected_tool_calls = _parse_tool_calls_from_text(response_text)

if detected_tool_calls:
    # Add tool_calls to the response message
    response_message.tool_calls = detected_tool_calls
```

# System Prompts

1. Comprehensive Tool Prompt Module (tool_prompts.py)
‚Ä¢  Base System Prompts: Foundational templates for AI assistant behavior
‚Ä¢  Detailed Tool Templates: Specific guidance for each tool (list_directory, read_file, search_files, write_file, edit_file)
‚Ä¢  Usage Guidelines: 7 key principles for effective tool usage
‚Ä¢  Common Workflows: Pre-defined strategies for different task types:
‚Ä¢  Explore New Codebase
‚Ä¢  Debug Issues  
‚Ä¢  Add New Features
‚Ä¢  Code Refactoring
‚Ä¢  Error Handling Templates: Guidance for common failure scenarios

2. Enhanced Server Integration
‚Ä¢  Updated _create_system_prompt_with_tools(): Now uses the comprehensive prompt system
‚Ä¢  Contextual Prompt Generation: Analyzes user queries to provide relevant guidance
‚Ä¢  Tool Call Validation: Built-in format checking and error reporting

3. New API Endpoints for Testing & Demonstration

#### GET /v1/tools/prompt/demo
Demonstrates the tool prompt system capabilities:

```bash
# Basic demo
curl "http://localhost:8080/v1/tools/prompt/demo"

# Contextual demo
curl "http://localhost:8080/v1/tools/prompt/demo?user_query=I want to debug a Python error"
```

#### GET /v1/tools/prompt/generate
Generate custom tool prompts:

```bash
# Generate with custom tools
curl "http://localhost:8080/v1/tools/prompt/generate" \
  -G --data-urlencode 'tools_json=[{"type":"function","function":{"name":"custom_tool","description":"My tool"}}]'
```

4. Key Features

#### üéØ Contextual Prompts
The system analyzes user queries and provides contextually relevant guidance:
‚Ä¢  Debug queries ‚Üí Debug workflow + error handling tips
‚Ä¢  Exploration queries ‚Üí Codebase exploration strategies  
‚Ä¢  Feature requests ‚Üí Development best practices

#### üìö Comprehensive Tool Documentation
Each tool includes:
‚Ä¢  When to use: Clear use case scenarios
‚Ä¢  Best practices: Expert guidance for effective usage
‚Ä¢  Examples: Real-world usage patterns with explanations
‚Ä¢  Parameter details: Complete API documentation

#### üîÑ Common Workflows 
Pre-built strategies for:
‚Ä¢  First-time codebase exploration
‚Ä¢  Systematic debugging approaches
‚Ä¢  Feature development workflows 
‚Ä¢  Code refactoring methodologies

#### üõ†Ô∏è Error Recovery
Built-in guidance for handling:
‚Ä¢  File not found errors
‚Ä¢  Permission issues
‚Ä¢  Search failures
‚Ä¢  Edit conflicts

5. Usage in Chat Completions

The enhanced system now automatically:
1. Detects tools in chat completion requests
2. Analyzes user queries for context
3. Generates appropriate prompts with relevant guidance
4. Provides structured tool calling instructions

6. Example Generated Prompt

When tools are provided, the system generates prompts like:

```
You are a helpful AI assistant with access to tools for exploring and modifying files within a code repository or documentation collection.

## Available Tools

### list_directory
**Description**: List files and directories in a specified path
**When to use**:
- When first exploring a new codebase
- To understand project structure and organization
**Best practices**:
- Start with the root directory to get overall structure
- Use recursive=true for deep exploration
**Examples**:
- First time exploring a project:
  {"function": "list_directory", "arguments": {"path": "."}}

[... comprehensive tool documentation continues ...]

## Tool Usage Guidelines
1. Always think before acting
2. Start broad, then narrow
3. Be systematic in exploration
[... continues with detailed guidance ...]
```

# Testing

1. ‚úÖ Tool execution tests - test_tool_execution.py
2. ‚úÖ OpenAI API compatibility tests - test_openai_compatibility.py
3. ‚úÖ Security and sandboxing tests - test_security_sandboxing.py
4. ‚úÖ Error handling tests - test_error_handling.py
5. ‚úÖ Integration test script - run_integration_tests.py

üìÅ Delivered Test Files:

1. tests/test_tool_execution.py (456 lines)
‚Ä¢  LLM response parsing (JSON, XML, function-like formats)
‚Ä¢  Tool call validation and execution with mock workspace
‚Ä¢  Realistic workflow scenarios (code exploration, debugging)
‚Ä¢  Integration tests with FastAPI TestClient
‚Ä¢  Mixed format response handling

2. tests/test_openai_compatibility.py (520 lines)  
‚Ä¢  Complete OpenAI API format validation
‚Ä¢  Request/response structure compliance
‚Ä¢  Streaming response testing
‚Ä¢  Tool choice parameter variations
‚Ä¢  Conversation flow validation
‚Ä¢  Parameter validation and error handling

3. tests/test_security_sandboxing.py (640 lines)
‚Ä¢  Path traversal attack prevention (basic, encoded, symlinks)
‚Ä¢  Execution limits and resource management
‚Ä¢  File permission security testing
‚Ä¢  Input sanitization (filenames, content, queries)
‚Ä¢  Workspace isolation and cleanup
‚Ä¢  Security error handling consistency

4. tests/test_error_handling.py (680 lines)
‚Ä¢  Tool execution errors (permissions, file not found, corruption)
‚Ä¢  LLM response parsing errors and malformed inputs
‚Ä¢  Recovery scenarios and partial failure handling
‚Ä¢  Edge cases (empty files, unicode, long filenames)
‚Ä¢  Concurrent access and memory pressure testing
‚Ä¢  API-level error handling

5. tests/run_integration_tests.py (600+ lines, executable)
‚Ä¢  Comprehensive integration test orchestrator
‚Ä¢  Parallel and sequential execution modes
‚Ä¢  Detailed reporting with timing and statistics
‚Ä¢  System integration validation
‚Ä¢  Coverage report generation
‚Ä¢  Advanced debugging and cleanup options

6. tests/README.md (200+ lines)
‚Ä¢  Complete documentation of the test suite
‚Ä¢  Usage instructions and examples
‚Ä¢  Test coverage overview
‚Ä¢  Debugging guides and CI integration

üîß Key Features Implemented:

Testing Infrastructure:
‚Ä¢  Isolated temporary workspaces for each test
‚Ä¢  Mock file system operations with security controls
‚Ä¢  FastAPI TestClient integration
‚Ä¢  Parallel test execution with thread safety
‚Ä¢  Comprehensive reporting and logging

Security Testing:
‚Ä¢  Path traversal protection (basic, URL-encoded, symlinks)
‚Ä¢  File size and execution limits
‚Ä¢  Input sanitization and validation
‚Ä¢  Resource management and cleanup
‚Ä¢  Permission-based access control

Compatibility Testing:
‚Ä¢  Full OpenAI API format compliance
‚Ä¢  Request/response structure validation
‚Ä¢  Streaming response handling
‚Ä¢  Tool choice parameter variations
‚Ä¢  Conversation flow testing

Error Handling:
‚Ä¢  Graceful error recovery
‚Ä¢  Malformed input handling  
‚Ä¢  Concurrent access scenarios
‚Ä¢  Edge case and boundary testing
‚Ä¢  Resource exhaustion handling

Integration & Automation:
‚Ä¢  End-to-end system validation
‚Ä¢  Automated test suite execution
‚Ä¢  CI/CD ready with proper exit codes
‚Ä¢  Coverage report generation
‚Ä¢  Detailed failure reporting

üöÄ Ready to Use:

The test suite is immediately usable:

```bash
# Run all tests
./tests/run_integration_tests.py

# Run with coverage
./tests/run_integration_tests.py --coverage

# Individual test suites  
pytest tests/test_tool_execution.py -v
```

